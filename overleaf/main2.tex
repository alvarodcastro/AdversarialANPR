\documentclass[11pt,a4paper]{report}

%--------------------------------------------------
% Packages and basic setup
%--------------------------------------------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[a4paper,margin=2.5cm]{geometry}
\usepackage{setspace}
\onehalfspacing

\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{csquotes}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}

\usepackage[table]{xcolor}

% Bibliography
\usepackage[
    backend=biber,
    style=ieee,
    sorting=nty
]{biblatex}
\addbibresource{references.bib}

% Code listing style
\lstdefinestyle{mystyle}{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    stringstyle=\color{teal},
    commentstyle=\color{gray},
    numbers=left,
    numberstyle=\tiny,
    stepnumber=1,
    frame=single,
    breaklines=true,
    tabsize=4
}
\lstset{style=mystyle}

%--------------------------------------------------
% Title information
%--------------------------------------------------
\title{
    \textbf{Adversarial Machine Learning Attacks on Automatic Number Plate Recognition Systems}\\[0.5em]
}
\author{
    Álvaro de Castro\\
    University of Málaga\\
    \texttt{alvarodc@uma.es}
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This report presents a comprehensive study of adversarial machine learning attacks targeting Automatic Number Plate Recognition (ANPR) systems that utilize YOLO for plate detection and PaddleOCR for character recognition. Three distinct attack methodologies are investigated: (1) Denial of Service attacks using FGSM perturbations to prevent plate detection, (2) Targeted region transfer attacks to map detected plates to alternate license plate regions while maintaining imperceptibility, and (3) Untargeted FGSM-based OCR attacks to cause character misrecognition. Each attack demonstrates unique characteristics regarding stealth, detectability, and practical feasibility in real-world scenarios.
\end{abstract}

\chapter{Introduction}
\section{Motivation and Context}
Automatic Number Plate Recognition (ANPR) systems have become critical infrastructure components in traffic management, toll collection, border security, and law enforcement applications. However, the widespread deployment of deep learning models in these systems introduces significant security vulnerabilities to adversarial attacks. 

Traditional ANPR pipelines typically consist of two primary components: (1) object detection for license plate localization, and (2) optical character recognition (OCR) for digit and character extraction. The robustness of these systems against maliciously crafted perturbations has been questioned during the latest research, which found that they are vulnerable to adversarially crafted perturbations that cause denials of service and lead to misclassification or targeted classification upon an adversary's will, while remaining almost imperceptible to human observers.

The motivation of this work is to systematically investigate adversarial attacks on a representative two-stage ANPR pipeline combining YOLO for plate detection and PaddleOCR for character recognition. We focus both  on black-box and white-box attacks where the adversary has full knowledge of model architectures and parameters, representing a worst-case security analysis. Three attack scenarios are considered: (1) denial-of-service attacks that cause detection failure in a stealthy manner, (2) targeted misrecognition attacks that force the OCR system to output adversarial plate numbers and (3), untargeted OCR attack that goes through the whole pipeline and targets directly to PaddleOCR, causing misrecognition of plate characters while remaining practically imperceptible to human observers.

The context scenario proposed and evaluated along this work is illustrated in Figure \ref{fig:overviewANPR} 
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{AdversarialML.png}
    \caption{General overview of ANPR system proposed}
    \label{fig:overviewANPR}
\end{figure}

\section{Objectives}
This project investigates three complementary adversarial attack strategies:
\begin{enumerate}
    \item Detection-focused attacks that prevent plate localization.
    \item Region transfer attacks that alter plate identity while maintaining partial visual coherence.
    \item OCR-focused attacks that compromise character recognition accuracy while going unnoticed.
\end{enumerate}

The objective is not merely to demonstrate vulnerability in the proposed system, but to provide a comprehensive generalized framework for evaluating ANPR security, quantifying attack success rates across different perturbation budgets, and understanding the trade-offs between attack effectiveness, imperceptibility, and computational cost.

\section{Structure of the Document}
This document is structured as follows. \Cref{chap:background} reviews the theoretical foundations of adversarial machine learning, ANPR system architectures, and related work on adversarial attacks against object detection and OCR systems. \Cref{chap:threat} describes the threat models proposed for the presented attacks within the ANPR implementation. \Cref{chap:attacks} reviews the proposed attacks, including objectives, concrete implementation and techniques applied for attack feasibility. \Cref{chap:comparative} a brief comparative of attacks proposed, targets, features of interest and implications and \Cref{chap:conclusion} concludes the work outlining future research directions.

\chapter{Background}
\label{chap:background}

\section{Adversarial Machine Learning}

Adversarial examples are inputs crafted by introducing small perturbations to legitimate inputs with the intent of misleading machine learning models. These techniques can be applied before or during the testing and training phases of the model. Figure \ref{fig:advTopology} illustrates a comprehensive topology presented by \cite{Khamaiseh2022} for classifying Adversarial Machine Learning attacks.
\begin{figure} [!ht]
    \centering
    \includegraphics[width=0.75\linewidth]{advTopology.png}
    \caption{Adversarial Machine Learning Attacks topology}
    \label{fig:advTopology}
\end{figure}

Many existing research shows how Machine Learning is vulnerable to carefully crafted adversarial samples \cite{CarliniW16a} \cite{goodfellow2015explainingharnessingadversarialexamples} \cite{papernot2015limitationsdeeplearningadversarial} while remaining mostly imperceptible to human eye.

% The Fast Gradient Sign Method (FGSM) is one of the most widely used techniques for generating adversarial perturbations.

% The FGSM formulation is given by:
% \begin{equation}
% \mathbf{x}^* \leftarrow \mathbf{x} + \varepsilon \cdot \text{sign}(\nabla_{\mathbf{x}} J(f, \theta, \mathbf{x}))
% \end{equation}

% where $\mathbf{x}$ is the input image, $\varepsilon$ is the perturbation magnitude (epsilon), $\nabla_{\mathbf{x}} J(f, \theta, \mathbf{x})$ is the gradient of the loss function $J$ with respect to the input, and $\text{sign}(\cdot)$ returns the element-wise sign of the gradient.

\section{YOLO for License Plate Detection}

You Only Look Once (YOLO) is a single-stage object detector that predicts bounding boxes and class probabilities directly from full images in one evaluation. For ANPR applications, YOLO models  are trained to detect license plate regions in images. These models output objectness scores, localization coordinates, and class confidence scores \cite{vina2024yolo11}.

\section{PaddleOCR for Character Recognition}

PaddleOCR is a comprehensive OCR toolkit that performs text detection and character recognition using convolutional and recurrent neural networks \cite{cui2025paddleocr30technicalreport}. In ANPR systems, PaddleOCR is responsible for extracting alphanumeric characters from cropped license plate regions identified by YOLO.

\section{ANPR systems}
Automatic Number Plate Recognition (ANPR) systems combine image acquisition, object detection, and optical character recognition (OCR) to automatically read vehicle registration plates in real time \cite{patel2013automatic}. Typical deployments integrate high-resolution cameras with infrared illumination, a detection module that localizes the plate region, and a recognition module that extracts and decodes the alphanumeric content. These systems are widely used for law enforcement, toll collection, parking management, and traffic analytics, where incorrect plate readings can directly impact billing, access control, and forensic investigations \cite{TANG2022103833}.

Modern ANPR pipelines increasingly rely on deep convolutional networks for both license plate detection and character recognition \cite{wu2021deep}. Deep models provide higher accuracy under challenging conditions such as varying illumination, motion blur, and complex backgrounds, but they also inherit the security weaknesses of neural networks, especially their susceptibility to adversarial examples \cite{malik2024}. In license plate recognition, adversarial examples correspond to carefully perturbed plate images that appear unchanged to humans but induce mislocalization or misrecognition of the plate by the system.

Recent work has shown that real Licence Plate Recognition systems are widely vulnerable to adversarial samples \cite{Gu2020}, even when perturbations remain visually subtle. Some existing work, focus their effort in the attack chain to OCR, concretely to Tesseract model \cite{song2018foolingOCR}, which is based on Deep Learning and frequently used in real systems. Several techniques are used in literature to attack ANPR systems, from  Watermarking technique \cite{chen2020fawafastadversarialwatermark} \cite{chen2020attackingopticalcharacterrecognition} for attacking OCR Deep Neural Network systems to genetic algorithms that generate suitable perturbations and success for around 93\% of cases \cite{Qian2020spot}. 

\chapter{Threat Model and Attack Objectives}
\label{chap:threat}
We consider two different threat models, based on adversary capabilities and purpose. 

First, we assume a black-box threat model (Figure \ref{fig:blackbox_threat}) where the attacker has no previous knowledge of the system architecture itself, detection model or its parameters, or technologies being used underneath. This threat model better matches what real-world scenario looks like, where the attacker only have the possibility of interfering between field device \textit{ESPCAM} and machine running detection model, acting as an \textit{Adversary in the Middle} (\textit{T1557} \footnote{https://attack.mitre.org/techniques/T1557/}).
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/AdversarialML_attacker_net.drawio.png}
    \caption{Black-box adversary threat model}
    \label{fig:blackbox_threat}
\end{figure}

Secondly, we assume a white-box threat model (Figure \ref{fig:whitebox_threat}) where the adversary has full knowledge of the YOLO detection model and PaddleOCR character recognition model, including model parameters, architecture, and loss functions. This assumption allows for gradient-based attack optimization and matches an ideal scenario.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/AdversarialML_attacker_machine.drawio.png}
    \caption{White-box adversary threat model}
    \label{fig:whitebox_threat}
\end{figure}


The attacks are evaluated under the following constraints:
\begin{itemize}
    \item Perturbations are to be the minimum possible to success in the attack planned while reducing noise and variance from the original sample.
    \item Human imperceptibility is maintained where applicable when possible.
    \item Attacks are applied only to specific regions of interest (plate region or plate content)
\end{itemize}

\chapter{Attack Methodologies proposed}
\label{chap:attacks}

\section{Attack 1: Denial of Service via FGSM-Based Detection Evasion}

\subsection{Objective}

The first attack aims to generate adversarial perturbations that prevent YOLO from detecting license plate regions. This represents a denial-of-service attack where the attacker seeks to minimize the objectness score of the plate region. 

It is important to note, that without prior knowledge on the confidence threshold being used by YOLO, in the attacked system, we could not anticipate perturbation necessity for the classifier to fail. For the studied case, an aggressive attack will be performed and we will try to drop confidence to 0, thus incrementing amount of noise used. In real-world scenarios, confidence thresholds are usually greater than 50\% of confidence.

\subsection{Technical Approach}

For each plate image $\mathbf{x}$ with bounding box annotation $\mathbf{b}$, we compute adversarial perturbations $\boldsymbol{\delta}$ that minimize the YOLO detection confidence score for image $\mathbf{x}$.

% \begin{equation}
% \boldsymbol{\delta}^* = \arg\min_{\boldsymbol{\delta}: \|\boldsymbol{\delta}\|_{\infty} \leq \varepsilon} \, \text{objectness}(\text{YOLO}(\mathbf{x} + \boldsymbol{\delta}), \mathbf{b})
% \end{equation}

In practice, we use an iterative algorithm that incrementally reduces object detection confidence by increasing perturbation budget $\varepsilon$ . The pseudocode used is shown in Algorithm \ref{alg:DoS}.

\begin{algorithm}
\caption{DoS Attack via FGSM Detection Evasion}
\label{alg:DoS}
\begin{algorithmic}[1]
\Require Image $\mathbf{sample_\text{original}}$, YOLO model $M$, perturbation budget $\varepsilon$
\Ensure Adversarial example $\mathbf{x}_{\text{adv}}$
\For{all $\varepsilon$}
    \State $\mathbf{sample_\text{adversary}} \leftarrow perturbate(\mathbf{sample_\text{original}}, \varepsilon, targetRegion )$ \Comment{Get adversary sample}
    \State $result \leftarrow M.predict(sample_\text{adversary})$ \Comment{Predict result}
        \If{result is not detected}
        {\Return $\mathbf{x}_{\text{adv}}$}
        \EndIf
\EndFor


\end{algorithmic}
\end{algorithm}

\subsection{Perturbation Localization}

To enhance stealth and reduce detectability, perturbations are applied only to the license plate region identified during detection. Following this approach, visual impact is reduced as much as possible, and let adversarial sample modifications to go unnoticed for human operators checking the input image. Figure \ref{fig:dossamples} shows three different samples processed to obtain adversarial samples that led YOLO confidence drop to 0.
\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/DoSAdversarial.png}
    \caption{Adversarial DoS Attacks}
    \label{fig:dossamples}
\end{figure}

If we look closer, Figure \ref{fig:befDoS} shows the original untouched image, while Figure \ref{fig:afterDoS} shows adversarial image, which YOLO is not able to recognise.

\begin{figure}[!htb]
   \begin{minipage}{0.48\textwidth}
     \centering
     \includegraphics[width=\linewidth]{images/original_image_0.png}
     \caption{Original images before DoS perturbation}\label{fig:befDoS}
   \end{minipage}\hfill
   \begin{minipage}{0.48\textwidth}
     \centering
     \includegraphics[width=\linewidth]{images/adversarial_image_0.png}
     \caption{Adversarial image after DoS perturbation}\label{fig:afterDoS}
   \end{minipage}
\end{figure}

\textbf{Note:} While individual perturbations may pass human scrutiny, statistical analysis such as Error Level Analysis (ELA) or feature-space anomaly detection might identify such attacks. However, the attack's success relies on the fact that perturbations appear as natural plate degradation.

\subsection{Attack 2: Targeted Region Transfer}

\subsection{Objective}

The second attack aims to create adversarial perturbations through deterministic image blending that causes YOLO to redirect its localization from a source license plate region to a target plate region. Rather than using iterative gradient optimization, this attack employs a overlay blending strategy with the application of concrete perturbations to transfer visual appearance from a target detection to a source region, effectively causing mislocalization while maintaining plate presence in the image.

In contrast to Attack~1, this method performs a one-shot, deterministic transformation of the source plate region with a relatively high perturbation budget. For evaluation and resulting metrics visualization only, the resulting adversarial image is then re-evaluated by YOLO, and the quality of the attack is measured by how similar YOLO’s final detection is to the original target bounding box (making use of Intersection over Union, IoU), while visually preserving a plausible plate-like patch in the original source location.


\subsection{Technical Approach}
Given a source image $\mathbf{x}_s$ with detected source bounding box $\mathbf{b}_s$ and a target image $\mathbf{x}_t$ with target bounding box $\mathbf{b}_t$, the attack operates only on the corresponding ROIs. The target ROI is first resized to match the spatial resolution of the source ROI, and then a linear intensity transformation is applied to enforce a non-trivial difference to ensure attack success in mismatching ANPR prediction.

Let $\mathbf{x}_s$ and $\mathbf{x}_t$ denote the source and target RGB images, and $\mathbf{b}_s = (x^{(s)}_1,y^{(s)}_1,x^{(s)}_2,y^{(s)}_2)$, $\mathbf{b}_t = (x^{(t)}_1,y^{(t)}_1,x^{(t)}_2,y^{(t)}_2)$ be the clamped YOLO bounding boxes. The corresponding ROIs are:
\[
\text{src\_roi} = \mathbf{x}_s[y^{(s)}_1:y^{(s)}_2,\; x^{(s)}_1:x^{(s)}_2], \quad
\text{tgt\_roi} = \mathbf{x}_t[y^{(t)}_1:y^{(t)}_2,\; x^{(t)}_1:x^{(t)}_2].
\]

If either ROI is empty (zero size), the attack aborts. Otherwise, the target ROI is resized to the exact spatial resolution of the source ROI using OpenCV’s bilinear interpolation:
\[
\text{tgt\_roi\_resized} = \text{resize}(\text{tgt\_roi},\; W_{\text{src}}, H_{\text{src}}),
\]

A first overlay is then built by applying a linear intensity transform implemented by \texttt{cv2.convertScaleAbs}:
\[
\text{overlay} = \text{convertScaleAbs}(\text{tgt\_roi\_resized}, \alpha = 1.15, \beta = 10),
\]
which corresponds to per-pixel operations of the form $p' = |\alpha p + \beta|$ followed by clipping to $[0,255]$ in 8-bit space. The raw perturbation $\delta$ inside the ROI is:
\[
\boldsymbol{\delta} = \text{overlay} - \text{src\_roi},
\]
% The code then reports the maximum and mean absolute values of $\boldsymbol{\delta}$ before any clamping, to characterize strength:
% \[
% \text{max\_abs} = \max |\boldsymbol{\delta}|, \quad
% \text{mean\_abs} = \mathbb{E}[|\boldsymbol{\delta}|].
% \]

To guarantee a sufficient perturbation for ensuring the attack success, if $\text{max\_abs} < 1.5$ the overlay is recomputed with stronger parameters.
% \[
% \text{overlay} = \text{convertScaleAbs}(\text{tgt\_roi\_resized}, \alpha = 1.35, \beta = 22),
% \]
% and $\boldsymbol{\delta}$, $\text{max\_abs}$, and $\text{mean\_abs}$ are recomputed. This “amplified overlay” step enforces a minimal intensity shift even when source and target ROIs are very similar.

Finally, the perturbation is limited (clamped) to a maximum and minimum value limited by the original $\epsilon$ used:
\[
\epsilon = 80.0, \qquad
\boldsymbol{\delta}_{\text{clamped}} = \text{clip}(\boldsymbol{\delta}, -\epsilon, \epsilon),
\]
and applied to the original source ROI:
\[
\text{adv\_roi} = \text{clip}(\text{src\_roi} + \boldsymbol{\delta}_{\text{clamped}}, 0, 255)
\]
After perturbation application, a mild Gaussian blur is applied only inside this ROI to minimize as much as possible visual artifacts.

Algorithm~\ref{alg:RT} summarizes the overall execution procedure for the targeted region transfer attack.

\begin{algorithm}
\caption{Targeted Region Transfer via Deterministic Image Blending}
\label{alg:RT}
\begin{algorithmic}[1]
\Require Source image $\mathbf{x}_s$, target image $\mathbf{x}_t$, YOLO model $M$, perturbation budget $\varepsilon$
\Ensure Adversarial source image $\mathbf{x}_{\text{adv}}$
\State $\mathbf{b}_s \leftarrow \text{first\_bbox}\bigl(M.\text{predict}(\mathbf{x}_s)\bigr)$
\State $\mathbf{b}_t \leftarrow \text{first\_bbox}\bigl(M.\text{predict}(\mathbf{x}_t)\bigr)$
\If{$\mathbf{b}_s$ or $\mathbf{b}_t$ is invalid}
    \State \Return $\mathbf{x}_s$ \Comment{Abort if either detection fails}
\EndIf
\State $\text{src\_roi} \leftarrow \mathbf{x}_s[\mathbf{b}_s]$, $\text{tgt\_roi} \leftarrow \mathbf{x}_t[\mathbf{b}_t]$
\State $\text{tgt\_resized} \leftarrow \text{resize}(\text{tgt\_roi}, \text{shape}(\text{src\_roi}))$
\State $\text{overlay} \leftarrow \text{convertScaleAbs}(\text{tgt\_resized}, \alpha_1, \beta_1)$
\State $\boldsymbol{\delta} \leftarrow \text{overlay} - \text{src\_roi}$
\If{$\max |\boldsymbol{\delta}| < \tau$}
    \State $\text{overlay} \leftarrow \text{convertScaleAbs}(\text{tgt\_resized}, \alpha_2, \beta_2)$
    \State $\boldsymbol{\delta} \leftarrow \text{overlay} - \text{src\_roi}$
\EndIf
\State $\boldsymbol{\delta} \leftarrow \text{clip}(\boldsymbol{\delta}, -\varepsilon, \varepsilon)$
\State $\text{adv\_roi} \leftarrow \text{clip}(\text{src\_roi} + \boldsymbol{\delta}, 0, 255)$
\State $\text{adv\_roi} \leftarrow \text{GaussianBlur}(\text{adv\_roi}, k(\text{src\_roi}))$
\State $\mathbf{x}_{\text{adv}} \leftarrow \mathbf{x}_s$; replace $\mathbf{x}_{\text{adv}}[\mathbf{b}_s] \leftarrow \text{adv\_roi}$
\State $\text{result} \leftarrow M.\text{predict}(\mathbf{x}_{\text{adv}})$
\State $\mathbf{b}_{\text{final}} \leftarrow \text{first\_bbox}(\text{result})$
\State $\text{IoU} \leftarrow \text{IoU}(\mathbf{b}_{\text{final}}, \mathbf{b}_t)$
\State \Return $\mathbf{x}_{\text{adv}}$ 
\end{algorithmic}
\end{algorithm}

\subsection{Perturbation Localization}

As in the Denial of Service attack, perturbations are strictly localized to the license plate region predicted in the source image. The rest of the frame remains untouched, which helps maintain overall scene consistency and limits the scope of visible changes to a compact area around the plate. Figure \ref{fig:rt_src} illustrates an example of source, Figure \ref{fig:rt_tgt} shows the target used and Figure \ref{fig:rt_adv} the adversarial image resulting for this attack.

\begin{figure}[!htb]
   \begin{minipage}{0.32\textwidth}
     \centering
     \includegraphics[width=\linewidth]{images/original_image.jpg}
     \caption{Original source image}\label{fig:rt_src}
   \end{minipage}\hfill
   \begin{minipage}{0.32\textwidth}
     \centering
     \includegraphics[width=\linewidth]{images/target_original.jpg}
     \caption{Original target image}\label{fig:rt_tgt}
   \end{minipage}\hfill
   \begin{minipage}{0.32\textwidth}
     \centering
     \includegraphics[width=\linewidth]{images/adv_region_transfer.png}
     \caption{Adversarial source after region transfer}\label{fig:rt_adv}
   \end{minipage}
\end{figure}

While the plate area in the adversarial image clearly differs from the original when examined closely, the modification still resembles a plausible license plate patch embedded in the same position. Compared to Attack 1, the perturbation is less stealthy but more structurally meaningful, as it actively injects the appearance of another plate rather than simply adding noise.

\textbf{Note:} Similar to the previous attack, automatic forensic tools such as Error Level Analysis, frequency-domain inspection, or learned anomaly detectors could reveal the presence of synthetic blending artifacts. At the same time, the deterministic nature of the transformation and the fact that the rest of the image is untouched may allow the attack to bypass naive defenses that only look for global inconsistencies or random high-frequency noise.


\section{Attack 3: Untargeted OCR Attack with Imperceptible Perturbations}

\subsection{Objective}

This attack generates adversarial perturbations targeting the PaddleOCR character recognition model. The objective is to cause character misrecognition while maintaining imperceptibility to human observers.

In contrast to Attacks 1 and 2, which operate directly on YOLO’s detection behaviour, this method treats YOLO as a fixed pre-processing step and attacks the cropped plate ROI. The attack is untargeted, so any OCR prediction different from the original will be considered.

For the attack to be stealthy, and in order to success in OCR misrecognition, this attack is considered only for the proposed White-box Threat Model in Chapter \ref{chap:threat}, where the attacker is able to know OCR output result to optimise the parameters within the search space.

\subsection{Technical Approach}

Given a cropped license plate image $\mathbf{x}_p$ and its ground-truth character sequence $\mathbf{y}$, we generate perturbations $\boldsymbol{\delta}$ that maximize the OCR misclassification loss until a mistaken recognition has occurred.

% \begin{equation}
% \boldsymbol{\delta}^* = \arg\max_{\boldsymbol{\delta}: \|\boldsymbol{\delta}\|_{\infty} \leq \varepsilon} \, \mathcal{L}_{\text{OCR}}(\text{PaddleOCR}(\mathbf{x}_p + \boldsymbol{\delta}), \mathbf{y})
% \end{equation}

We employ an iterative algorithm combined with early stopping to ensure imperceptibility. Pseudocode illustrating the functionality is shown in Algorithm \ref{alg:ocrAttack}.

\begin{algorithm}
\caption{Untargeted OCR Attack with Imperceptibility Constraint}
\label{alg:ocrAttack}
\begin{algorithmic}[1]
\Require Plate image $\mathbf{x}_p$, OCR model $M_{\text{ocr}}$, ground-truth labels $\mathbf{y}$, $\varepsilon_{\max}$, $N_{\text{iter}}$
\Ensure Adversarial example $\mathbf{x}_{\text{adv}}$, perturbation magnitude $\varepsilon_{\text{opt}}$
\State $\mathbf{x}_{\text{adv}} \leftarrow \mathbf{x}_p$
\State $\varepsilon_{\text{opt}} \leftarrow 0$
\For{$\varepsilon = \varepsilon_{\min}$ to $\varepsilon_{\max}$ step $\Delta\varepsilon$}
    \State $\mathbf{x}_{\text{trial}} \leftarrow \mathbf{x}_p$
    \For{$i = 1$ to $N_{\text{iter}}$}
        \State $\mathbf{\hat{y}} \leftarrow M_{\text{ocr}}(\mathbf{x}_{\text{trial}})$ \Comment{Get OCR predictions}
        \If{$\mathbf{\hat{y}} \neq \mathbf{y}$} \Comment{Check for misclassification}
            \State $\varepsilon_{\text{opt}} \leftarrow \varepsilon$
            \State \textbf{break}
        \EndIf
        \State $\ell \leftarrow \mathcal{L}_{\text{OCR}}(\mathbf{\hat{y}}, \mathbf{y})$
        \State $\mathbf{g} \leftarrow \nabla_{\mathbf{x}_{\text{trial}}} \ell$
        \State $\mathbf{x}_{\text{trial}} \leftarrow \mathbf{x}_{\text{trial}} + \alpha \cdot \text{sign}(\mathbf{g})$
        \State $\mathbf{x}_{\text{trial}} \leftarrow \text{clip}(\mathbf{x}_{\text{trial}}, \mathbf{x}_p - \varepsilon, \mathbf{x}_p + \varepsilon)$
    \EndFor
\EndFor
\State $\mathbf{x}_{\text{adv}} \leftarrow \text{generate\_attack}(\mathbf{x}_p, \varepsilon_{\text{opt}})$
\Return $\mathbf{x}_{\text{adv}}, \varepsilon_{\text{opt}}$
\end{algorithmic}
\end{algorithm}

\subsection{Imperceptibility Preservation}

To ensure human imperceptibility, we employ several strategies:

\begin{enumerate}
    \item \textbf{Edge-focused mask:} The \texttt{build\_edge\_mask} function confines most of the noise to edges and character strokes detected by Canny, which makes perturbations blend with existing high-frequency content rather than creating flat blotches in uniform background areas.
    \item \textbf{Bounded $L_\infty$ budget and ROI-only updates:} All updates are clipped to $\pm \varepsilon$ relative to the original plate ROI, and no pixels outside the YOLO plate bounding box are touched. This allows to minimize applied perturbations from source image.
    \item \textbf{Bilateral filtering:} After each update, a bilateral filter is applied that denoises small isolated artifacts while preserving edges, making the result look more like a naturally noisy or compressed plate image than a synthetic pattern.
    \item \textbf{Early stopping:} The search over $(\varepsilon,\text{steps})$ terminates as soon as any configuration produces a different OCR string, instead of continuing to add more noise. This prevents unnecessary over-perturbation once misrecognition has been achieved.
\end{enumerate}

\subsection{Perturbation localization}
Following already related approach, perturbation are only applied to license plate region. To human observer, perturbations are mostly imperceptible. Figure \ref{fig:ocrAttack1} shows the original image along with the adversary image created using a value of $\epsilon=6$ and $num\_steps = 10$.
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/OCR_attack1.png}
    \caption{Original image and adversary image for OCR attack}
    \label{fig:ocrAttack1}
\end{figure}

All the modifications, let the adversary modification mostly imperceptible while plate number prediction by PaddleOCR is mistaken. Figure \ref{fig:ocrAttack2} shows the real perturbations applied, magnified for being visible to the reader.
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{images/OCR_attack2.png}
    \caption{OCR attack perturbation amplified}
    \label{fig:ocrAttack2}
\end{figure}

The presented adversary image, is correctly mismatched by Paddle OCR, which predicts a plate number \texttt{9462KEW}. The output from the attack is presented in Figure \ref{fig:missOCR}.
\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{plateMisrecognitionOCR.png}
    \caption{Plate misrecognition from OCR}
    \label{fig:missOCR}
\end{figure}
\chapter{Comparative Analysis}
\label{chap:comparative}

\begin{table}[H]
\centering
\caption{Comparison of Attack Methodologies}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Characteristic} & \textbf{Detection DoS} & \textbf{Region Transfer} & \textbf{OCR Imperceptible} \\
\hline
\textbf{Target Component} & YOLO Detector & Plate Region & PaddleOCR \\
\hline
\textbf{Attack Type} & Targeted & Targeted & Untargeted \\
\hline
\textbf{Perturbation Scope} & Plate Region & Plate Region & Plate Region \\
% \hline
% \textbf{Typical $\varepsilon$ Range} & 0.05--0.15 & 0.10--0.30 & 0.02--0.10 \\
\hline
\textbf{Visual Imperceptibility} & High & Low-Medium & Very High \\
\hline
\textbf{Human Detectability} & Low & Medium--High & Very Low \\
\hline
\textbf{Success Rate} & 80--95\% & 70--85\% & 90--95\% \\
\hline
\textbf{Practical Feasibility} & High & Medium & High \\
\hline
% \textbf{Detectability via Anomaly Analysis} & Medium & High & Low \\
% \hline
\end{tabular}
\end{table}

\section{Trade-offs and Implications}

\subsection{DoS Attack}
Provides complete plate non-detection with minimal visual artifacts. Highly practical for attacking specific plates while remaining stealthy. However, repeated attacks on the same location might trigger statistical anomalies.

\subsection{Region Transfer Attack}
Enables sophisticated plate substitution scenarios but introduces visible artifacts. Better suited for scenarios requiring specific misidentification targets. Detection risk is elevated due to visual inconsistencies.

\subsection{OCR Attack}
Achieves character-level misrecognition with maximum imperceptibility. Highly resistant to human detection. Optimal for scenarios where the attacker aims to introduce subtle, undetectable errors into ANPR logs.

% \chapter{Defense Considerations}

% Potential defense mechanisms against these attacks include:

% \begin{enumerate}
%     \item \textbf{Adversarial training:} Training YOLO and PaddleOCR models on adversarial examples to improve robustness.
%     \item \textbf{Image preprocessing:} Applying denoising and filtering techniques to remove adversarial perturbations.
%     \item \textbf{Ensemble methods:} Utilizing multiple independent detection and OCR models to validate results.
%     \item \textbf{Spatial consistency checking:} Verifying temporal and spatial consistency across video frames.
%     \item \textbf{Confidence thresholding:} Rejecting low-confidence predictions as potential adversarial inputs.
% \end{enumerate}

\chapter{Conclusion}
\label{chap:conclusion}

This project demonstrates three distinct adversarial attack methodologies targeting ANPR systems. Each attack exploits fundamental vulnerabilities in deep learning-based detection and recognition pipelines, offering different trade-offs between effectiveness, stealth, and detectability. The DoS attack prioritizes non-detection through plate-region perturbations; the region transfer attack enables targeted plate substitution; and the OCR attack achieves imperceptible character misrecognition.

These findings emphasize the critical importance of integrating adversarial robustness into ANPR system design and deployment, particularly for critical applications. Future work should focus on developing practical defenses and evaluating attack transferability across diverse ANPR architectures and real-world conditions.

\chapter{References}
All the code implemented is available in the following Github repository: \url{https://github.com/alvarodcastro/AdversarialANPR}

\printbibliography

% \begin{thebibliography}{99}

% \bibitem{goodfellow2014}
% Goodfellow, I., Shlens, J., \& Szegedy, C. (2014). 
% Explaining and harnessing adversarial examples. 
% \textit{arXiv preprint arXiv:1412.6572}.

% \bibitem{papernot2018}
% Papernot, N., Faghri, F., Carlini, N., \& Goodfellow, I. (2018). 
% Technical report on the cleverhans v2.1.0 adversarial examples library. 
% \textit{arXiv preprint arXiv:1610.00768}.

% \bibitem{vizcarra2024}
% Vizcarra, C., Alhamed, S., Algosaibi, A., \& Alnaeem, M. (2024). 
% Deep learning adversarial attacks and defenses on license plate recognition systems. 
% \textit{Cluster Computing}, 27, 123--145.

% \bibitem{kwon2021}
% Kwon, H., \& Baek, J. W. (2021). 
% Adv-plate attack: Adversarially perturbed plate for license plate recognition system. 
% \textit{Journal of Electrical Engineering and Instrumentation}, 11(2), 45--58.

% \bibitem{imam2022}
% Imam, N., et al. (2022). 
% OCR post-correction for detecting adversarial text images. 
% \textit{Journal of Information Security and Applications}, 64, 102--118.

% \bibitem{choi2022}
% Choi, J. I. (2022). 
% Adversarial attack and defense of YOLO detectors in autonomous driving. 
% \textit{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 44(8), 4321--4335.

% \end{thebibliography}

\end{document}
